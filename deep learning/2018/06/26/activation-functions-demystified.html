<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Activation Functions Demystified | Neuron</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Activation Functions Demystified" />
<meta name="author" content="Yashvardhan Jain" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An introduction to activation functions used in neural networks." />
<meta property="og:description" content="An introduction to activation functions used in neural networks." />
<link rel="canonical" href="https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html" />
<meta property="og:url" content="https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html" />
<meta property="og:site_name" content="Neuron" />
<meta property="og:image" content="https://j-yash.github.io/neuron/images/post2_main.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-06-26T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Yashvardhan Jain"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html"},"description":"An introduction to activation functions used in neural networks.","@type":"BlogPosting","url":"https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html","headline":"Activation Functions Demystified","dateModified":"2018-06-26T00:00:00-05:00","datePublished":"2018-06-26T00:00:00-05:00","image":"https://j-yash.github.io/neuron/images/post2_main.jpg","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/neuron/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://j-yash.github.io/neuron/feed.xml" title="Neuron" /><link rel="shortcut icon" type="image/x-icon" href="/neuron/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Activation Functions Demystified | Neuron</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Activation Functions Demystified" />
<meta name="author" content="Yashvardhan Jain" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An introduction to activation functions used in neural networks." />
<meta property="og:description" content="An introduction to activation functions used in neural networks." />
<link rel="canonical" href="https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html" />
<meta property="og:url" content="https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html" />
<meta property="og:site_name" content="Neuron" />
<meta property="og:image" content="https://j-yash.github.io/neuron/images/post2_main.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-06-26T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Yashvardhan Jain"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html"},"description":"An introduction to activation functions used in neural networks.","@type":"BlogPosting","url":"https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html","headline":"Activation Functions Demystified","dateModified":"2018-06-26T00:00:00-05:00","datePublished":"2018-06-26T00:00:00-05:00","image":"https://j-yash.github.io/neuron/images/post2_main.jpg","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://j-yash.github.io/neuron/feed.xml" title="Neuron" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/neuron/">Neuron</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/neuron/about/">About Me</a><a class="page-link" href="/neuron/search/">Search</a><a class="page-link" href="/neuron/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Activation Functions Demystified</h1><p class="page-description">An introduction to activation functions used in neural networks.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2018-06-26T00:00:00-05:00" itemprop="datePublished">
        Jun 26, 2018
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Yashvardhan Jain</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/neuron/categories/#deep learning">deep learning</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-06-26-activation-functions-demystified.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>With great deep learning resources, comes great deep learning jargon.</p>
</blockquote>
<p>If you’ve been studying deep learning for a while, you must have come across a lot of jargon associated with the field. And for beginners, it can feel pretty overwhelming. So, if you’ve come across the term “<strong>activation functions</strong>” and find yourself confused, don’t worry. I’ve got you covered.</p>
<p>Before we start talking about activation functions, let us first understand what exactly an activation is.</p>
<p><strong>So what is an activation?</strong></p>
<p>An activation is just a number. That’s all an activation is. As we know, the core of deep learning is the following equation:&gt; y = W * x + b
Once we calculate this equation, the value that <code>y</code> holds is called an activation. And so, every node in the neural network that calculates this equation, holds its own value of <code>y</code> and all those values are called activations. That is all there is to an activation. No magic. No complicated explanation. No hidden concepts. <strong>It’s simply a number</strong>.</p>
<p><strong>Alright, guy on the Internet. I get it. But what is an activation function then?</strong></p>
<p>An activation function is simply a mathematical function that is applied to an activation in order to introduce some <strong>non-linearity</strong> in our network. Confused? Read on.</p>
<p>A neural network is basically a combination of multiple linear and non-linear functions. The <code>y = m * x + b</code> is a linear function. But, if we only had linear functions in our network in all layers, then the network would simply act as a single layer network. Such a network would not be able to learn much.</p>
<p>That is why we use activation functions for producing the output of every node in every layer. This introduces non-linearity in our network which enables the network to learn.</p>
<blockquote><p>Such a combination of linear and non-linear functions makes a neural network capable of approximating anything.</p>
</blockquote>
<p><strong>Are there different activation functions that I can use?</strong></p>
<p>There are many different types of activation functions that have been used through the ages. But instead of going through all of them in this article, I will explain the 4 most popular and useful activation functions that are currently used in research as well as in the industry.</p>
<p><strong>Sigmoid():</strong> A sigmoid activation function turns an activation into a value between 0 and 1. It is useful for <strong>binary classification problems</strong> and is mostly used in the final output layer of such problems. Also, sigmoid activation leads to slow gradient descent because the slope is small for high and low values. A sigmoid activation is represented mathematically as the following equation:&gt; Sigmoid(z) = 1 / (1 + exp(-z))</p>
<p><img src="/neuron/images/copied_from_nb/../images/post2_sigmoid.png" alt="" title="Sigmoid activation" /></p>
<p><strong>Tanh():</strong> A Tanh activation function turns an activation into a value between -1 and +1. This activation function is better than the sigmoid activation function in most cases because the outputs are normalized. It is a really popular activation function that is used in the hidden layers. Mathematically, it is represented as:&gt; tanh(z) = [exp(z) - exp(-z) ]/[exp(z) + exp(-z)]</p>
<p><img src="/neuron/images/copied_from_nb/../images/post2_tanh.png" alt="" title="Tanh activation" /></p>
<p><strong>ReLU():</strong> A ReLU (Rectified Linear Unit) is a really complex and fancy sounding activation function, but all it does is simply turn negative values to zero. That’s all it does. It is <strong>the most</strong> popular and effective activation function and is used in the hidden layers. It is the most used activation function. And is the default choice for most neural network layers. Mathematically, it is represented as:&gt; ReLU(z) = max(0,z)</p>
<p><img src="/neuron/images/copied_from_nb/../images/post2_relu.png" alt="" title="ReLU activation" /></p>
<p><strong>Leaky ReLU:</strong> One problem with ReLU may be that the derivative(slope) for negative values is zero. In some cases, we may not want that. In order to combat that problem, we can use a leaky ReLU. A leaky ReLU makes sure that the slope for negative values is not zero. But mostly, ReLU function will just do fine. Mathematically, it is represented as:&gt; Leaky_ReLU(z) = max(0.01*z, z)</p>
<p><img src="/neuron/images/copied_from_nb/../images/post2_lrelu.png" alt="" title="Leaky ReLU activation" /></p>
<p>The <strong>advantage</strong> of ReLU and Leaky ReLU is that the derivative (slope) is much greater than zero (for z &gt; 0), and hence, the algorithm will learn much faster as compared to sigmoid or tanh.</p>
<p>So, these are the activation functions that are used everywhere from research to industry.</p>
<p><strong>But hey, how do I know which one to use?</strong></p>
<p>So, the rules of thumb for choosing activation functions are pretty simple:1. If you need to choose a value between 0 and 1, like in binary classification, then use a Sigmoid activation function. Otherwise, for all other cases, don’t use this.2. ReLU is the default choice for all the other cases. But in some cases, you can use tanh as well. Try playing with both, if you are not sure which one to use.</p>
<hr />
<p>That’s it about activation functions. They are used to make the network non-linear and you’ve got a few choices as to which activation function you can use. I hope I have successfully demystified activation functions for you and you understand them a bit better now.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/neuron/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/neuron/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/neuron/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An educational blog on AI, computer vision, deep learning, and autonomous vehicles.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/j-yash" title="j-yash"><svg class="svg-icon grey"><use xlink:href="/neuron/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
