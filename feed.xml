<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://j-yash.github.io/neuron/feed.xml" rel="self" type="application/atom+xml" /><link href="https://j-yash.github.io/neuron/" rel="alternate" type="text/html" /><updated>2020-07-25T11:40:56-05:00</updated><id>https://j-yash.github.io/neuron/feed.xml</id><title type="html">Neuron</title><subtitle>An educational blog on AI, computer vision, deep learning, and autonomous vehicles.</subtitle><entry><title type="html">Stoic Musings Archive</title><link href="https://j-yash.github.io/neuron/stoicism/non-technical/2020/06/30/Stoic-Musings-Archive.html" rel="alternate" type="text/html" title="Stoic Musings Archive" /><published>2020-06-30T00:00:00-05:00</published><updated>2020-06-30T00:00:00-05:00</updated><id>https://j-yash.github.io/neuron/stoicism/non-technical/2020/06/30/Stoic-Musings-Archive</id><content type="html" xml:base="https://j-yash.github.io/neuron/stoicism/non-technical/2020/06/30/Stoic-Musings-Archive.html">&lt;p&gt;&lt;img src=&quot;/neuron/images/stoic-musings-archive.jpg&quot; alt=&quot;&quot; title=&quot;Image Credit: Dariusz Sankowski from Unsplash&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the archive for my Stoic Musings challenge, inspired by the book “The Daily Stoic” by Ryan Holiday and Stephen Hanselman, where I take a quote from the book and reflect on it, every day for 366 days.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;July 1, 2020 - &lt;a href=&quot;https://medium.com/@jyash/my-stoic-musings-001-some-words-about-control-c7b48e45017e&quot;&gt;My Stoic Musings 001 - Some words about control&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 2, 2020 - &lt;a href=&quot;https://medium.com/@jyash/my-stoic-musings-002-education-will-set-us-free-2f8207c8b2eb&quot;&gt;My Stoic Musings 002 - Education will set us free&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 3, 2020 - &lt;a href=&quot;https://medium.com/@jyash/my-stoic-musings-003-ephemeral-time-290f64d26141&quot;&gt;My Stoic Musings 003 - Ephemeral Time&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 4, 2020 - &lt;a href=&quot;https://medium.com/@jyash/the-3-tenets-of-stoicism-my-stoic-musings-004-e28a8cd07a0b&quot;&gt;My Stoic Musings 004 - The 3 Tenets of Stoicism&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 5, 2020 - &lt;a href=&quot;https://medium.com/@jyash/saving-ourselves-from-the-mirage-my-stoic-musings-005-279d30f928c9&quot;&gt;My Stoic Musings 005 - Saving Ourselves from the Mirage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 6, 2020 - &lt;a href=&quot;https://medium.com/@jyash/start-with-why-my-stoic-musings-006-1d69ec45efbf&quot;&gt;My Stoic Musings 006 - Start With Why&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 7, 2020 - &lt;a href=&quot;https://medium.com/@jyash/7-things-for-a-stoic-mind-my-stoic-musings-007-bdeb990f22b2&quot;&gt;My Stoic Musings 007 - 7 Things for a Stoic Mind&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 8, 2020 - &lt;a href=&quot;https://medium.com/@jyash/what-seneca-had-to-say-about-addictions-in-everyday-life-my-stoic-musings-008-17ca93f29b94&quot;&gt;My Stoic Musings 008 - What Seneca Had To Say About Addictions In Everyday Life&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 9, 2020 - &lt;a href=&quot;https://medium.com/@jyash/what-really-sets-us-free-my-stoic-musings-009-a3274aa9fe36&quot;&gt;My Stoic Musings 009 - What Really Sets Us Free&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 10, 2020 - &lt;a href=&quot;https://medium.com/@jyash/on-choices-reason-good-and-evil-my-stoic-musings-010-22161fc25139&quot;&gt;My Stoic Musings 010 - On Choices, Reason, Good, and Evil&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 11, 2020 - &lt;a href=&quot;https://medium.com/@jyash/the-way-to-avoid-obstacles-is-to-not-focus-on-avoiding-them-my-stoic-musings-011-93c43dde8e0c&quot;&gt;My Stoic Musings 011 - The Way To Avoid Obstacles Is To Not Focus On Avoiding Them&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 12, 2020 - &lt;a href=&quot;https://medium.com/@jyash/maybe-our-freedom-to-choose-is-the-real-happiness-my-stoic-musings-012-219108a7df44&quot;&gt;My Stoic Musings 012 - Maybe Our Freedom To Choose Is The Real Happiness&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 13, 2020 - &lt;a href=&quot;https://medium.com/@jyash/the-illusion-we-have-about-what-we-can-control-my-stoic-musings-013-a5e07cfd70b8&quot;&gt;My Stoic Musings 013 - The Illusion We Have About What We Can Control&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 14, 2020 - &lt;a href=&quot;https://medium.com/@jyash/what-thoughts-now-occupy-my-mind-my-stoic-musings-01-4c2f82c166e7&quot;&gt;My Stoic Musings 014 - What Thoughts Now Occupy My Mind&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 15, 2020 - &lt;a href=&quot;https://medium.com/@jyash/clarity-is-the-only-path-to-peace-my-stoic-musings-015-a347b92c87&quot;&gt;My Stoic Musings 015 - Clarity Is The Only Path To Peace&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 16, 2020 - &lt;a href=&quot;https://medium.com/@jyash/our-habits-may-be-whats-holding-us-back-my-stoic-musings-016-70a7c0792ea5&quot;&gt;My Stoic Musings 016 - Our Habits May Be What’s Holding Us Back&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 17, 2020 - &lt;a href=&quot;https://medium.com/@jyash/if-epictetus-was-our-teacher-hed-say-this-my-stoic-musings-017-3c3c3444326&quot;&gt;My Stoic Musings 017 - If Epictetus Was Our Teacher, He’d Say This&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 18, 2020 - &lt;a href=&quot;https://medium.com/@jyash/live-right-so-you-can-die-right-my-stoic-musings-018-a3b357365262&quot;&gt;My Stoic Musings 018 - Live Right So You Can Die Right&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 19, 2020 - &lt;a href=&quot;https://medium.com/@jyash/the-choice-is-yours-my-stoic-musings-019-9f28fac330e7&quot;&gt;My Stoic Musings 019 - The Choice Is Yours&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 20, 2020 - &lt;a href=&quot;https://medium.com/@jyash/start-with-why-my-stoic-musings-020-2c1a6664ab6c&quot;&gt;My Stoic Musings 020 - Start With Why&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 21, 2020 - &lt;a href=&quot;https://medium.com/@jyash/meditate-my-stoic-musings-021-37ec3f8f1ed1&quot;&gt;My Stoic Musings 021 - Meditate On Your Actions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 22, 2020 - &lt;a href=&quot;https://medium.com/@jyash/dont-let-history-repeat-itself-my-stoic-musings-022-6acd00594054&quot;&gt;My Stoic Musings 022 - Don’t Let History Repeat Itself&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 23, 2020 - &lt;a href=&quot;https://medium.com/@jyash/whats-so-different-about-the-rich-my-stoic-musings-023-ea6ae307a98c&quot;&gt;My Stoic Musings 023 - What’s So Different About The Rich&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;July 24, 2020 - &lt;a href=&quot;https://medium.com/@jyash/why-we-must-scribble-in-our-books-my-stoic-musings-024-17e55bf8a8a9&quot;&gt;My Stoic Musings 024 - Why We Must Scribble In Our Books&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yashvardhan Jain</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://j-yash.github.io/neuron/images/stoic-musings-archive.jpg" /><media:content medium="image" url="https://j-yash.github.io/neuron/images/stoic-musings-archive.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Building The Hotdog/Not-Hotdog Classifier From HBO’s Silicon Valley</title><link href="https://j-yash.github.io/neuron/deep%20learning/builds/2018/08/30/hotdog-not-hotdog.html" rel="alternate" type="text/html" title="Building The Hotdog/Not-Hotdog Classifier From HBO's Silicon Valley" /><published>2018-08-30T00:00:00-05:00</published><updated>2018-08-30T00:00:00-05:00</updated><id>https://j-yash.github.io/neuron/deep%20learning/builds/2018/08/30/hotdog-not-hotdog</id><content type="html" xml:base="https://j-yash.github.io/neuron/deep%20learning/builds/2018/08/30/hotdog-not-hotdog.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-08-30-hotdog-not-hotdog.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Ever since I watched the &lt;a href=&quot;https://www.youtube.com/watch?v=ACmydtFDTGs&amp;amp;feature=youtu.be&quot;&gt;Hotdog/Not-Hotdog app&lt;/a&gt; created by the &lt;a href=&quot;https://www.hbo.com/silicon-valley/cast-and-crew/jian-yang&quot;&gt;wild-card app developer Jian-Yang&lt;/a&gt; in HBO’s Silicon Valley, I have wanted to create it. And now finally, I have! But due to my lack of app development skills, I created only the machine learning classifier that the app would use. Still, creating the brain of the app is pretty cool. Also, it’s a pretty cool Data Science project. So, here’s my experience.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Find the Jupyter Notebooks &lt;a href=&quot;https://github.com/J-Yash/Hotdog-Not-Hotdog&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;“It’s not magic. It’s talent and sweat.” — Gilfoyle&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Can&amp;#8217;t-Learn-Without-Data&quot;&gt;Can&amp;#8217;t Learn Without Data&lt;a class=&quot;anchor-link&quot; href=&quot;#Can&amp;#8217;t-Learn-Without-Data&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The first step of any data science or machine learning or deep learning or Artificial Intelligence project is getting the data. You can create the most sophisticated algorithms and run your ML models on the craziest GPUs or TPUs, but if your data is not good enough then you won’t be able to make any progress on your ML task. So, the first was to get the data.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;ImageNet to our Rescue!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I used the &lt;a href=&quot;http://image-net.org/&quot;&gt;ImageNet&lt;/a&gt; website to get the data. For the hotdogs, I searched for ‘hotdog’, ‘frankfurter’ and ‘chili-dog’. And for the ‘Not Hotdog’ part, I searched for ‘pets’, ‘furniture’, ‘people’ and ‘food’. The reason for choosing these was that these are the images people are most likely to take while using such an app.
I used a small script to download all these images from ImageNet and delete the invalid/broken images. Then, I went through all the images and manually deleted the broken and irrelevant images. Finally, I had 1822 images of hotdogs and 1822 images of “not hotdogs”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I had over 3000 images of “not hotdogs” but I randomly chose 1822 out of them in order to balance both the classes.&lt;/p&gt;
&lt;p&gt;Now, I had all the data that I needed to create this classifier.&lt;/p&gt;
&lt;h2 id=&quot;Make-Data-Better&quot;&gt;Make Data Better&lt;a class=&quot;anchor-link&quot; href=&quot;#Make-Data-Better&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The next step was to make the data better. First, I wrote a small script to resize all the images to 299x299 resolution so that all the images are of the same size and I wouldn’t have to hit various incompatibility problems while loading these images.&lt;/p&gt;
&lt;p&gt;It is also important to structure the data properly in the directories. I structured the data in the following way:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dataset —&amp;gt; (train —&amp;gt; (hotdog, nothotdog), test —&amp;gt; (hotdog, nothotdog), valid —&amp;gt; (hotdog, nothotdog))&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The reason for structuring the dataset this way is because Keras requires the data to be this way when we load it in Keras.&lt;/p&gt;
&lt;h2 id=&quot;First-Iteration-&amp;#8212;-Creating-a-Basic-ConV-Model&quot;&gt;First Iteration &amp;#8212; Creating a Basic ConV Model&lt;a class=&quot;anchor-link&quot; href=&quot;#First-Iteration-&amp;#8212;-Creating-a-Basic-ConV-Model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the first iteration, I created a simple Convolutional Model with two ConV layers and three fully-connected layers. Instead of loading the entire image dataset into the memory(numpy array), I used Keras generators to load the images during runtime in batches. I also used the &lt;code&gt;ImageDataGenerator&lt;/code&gt; Class in keras to augment the data in runtime in batches.&lt;/p&gt;
&lt;p&gt;This model took about 50 minutes to train with 50 epochs and gave an accuracy of 71.09% on the test set.&lt;/p&gt;
&lt;p&gt;This was a pretty low accuracy for such a simple task. I mean, human accuracy for this task has to be ~100%.
This got me thinking and I decided to take it up a notch for iteration #2.&lt;/p&gt;
&lt;h2 id=&quot;Second-Iteration-&amp;#8212;-Transfer-Learning&quot;&gt;Second Iteration &amp;#8212; Transfer Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Second-Iteration-&amp;#8212;-Transfer-Learning&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I decided to use Transfer Learning to make this classifier better and faster to train. Now, I couldn’t augment the data in real time because transfer learning doesn’t support that in Keras. So, I wrote a script to augment the data and create a better dataset. I applied the following transformations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Flip Horizontally&lt;/li&gt;
&lt;li&gt;Flip Vertically&lt;/li&gt;
&lt;li&gt;Rotate images at a certain angle&lt;/li&gt;
&lt;li&gt;Shear and Zoom images&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This data augmentation led to dataset version 2.0 which had 7822 hotdog images and 7822 “not hotdog” images. This was a much larger dataset so I expected some increase in the accuracy of the classifier. Now, it is much better to actually &lt;strong&gt;get new data&lt;/strong&gt; instead of &lt;strong&gt;applying data augmentation&lt;/strong&gt;, as taught by Andrew Ng in &lt;a href=&quot;https://www.deeplearning.ai/&quot;&gt;Deeplearning.ai&lt;/a&gt; course, but since the former wasn’t a choice, data augmentation can make &lt;strong&gt;at least some improvement&lt;/strong&gt; in the classifier.
Now that I had a relatively larger and better dataset, it was time for &lt;strong&gt;INCEPTION!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;No, not the one by Christopher Nolan.&lt;/p&gt;
&lt;p&gt;I decided to use the InceptionNet V3 because it provides better results than VGGNet and ResNet50. Keras provides an &lt;code&gt;Application&lt;/code&gt; module for using these pre-trained architectures quite easily. So, I downloaded the pre-trained InceptionV3 model and weights and then trained the dataset on this model to get the bottleneck features of the dataset. This is done by basically removing the fully-connected layers from the InceptionNet and training the data only on the ConV layers. This process converts our raw images into vectors.&lt;/p&gt;
&lt;p&gt;Now we create a simple ConV model(the same in iteration #1) and train it on these bottleneck features. This model took about 5 minutes to train and gave an accuracy of 96.42% on the test set. That’s a pretty good accuracy for such less data. Since accuracy is not always the best metric to measure a model’s performance, I created a confusion matrix.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/neuron/images/copied_from_nb/../images/post3_plot.png&quot; alt=&quot;&quot; title=&quot;Confusion Plot&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the images that were incorrectly classified, many of them that the classifier incorrectly classified as “not hotdog” didn’t really have a good view of the hotdog. Or had more than just a hotdog. Although, why it classified a group of people as a hotdog is a mystery I wish to solve one day!&lt;/p&gt;
&lt;h2 id=&quot;Wait,-what-did-we-just-do?&quot;&gt;Wait, what did we just do?&lt;a class=&quot;anchor-link&quot; href=&quot;#Wait,-what-did-we-just-do?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;We took an idea from a TV show and decided to go through the entire ML process using that idea.&lt;/li&gt;
&lt;li&gt;First, we collected the data because that’s what powers our ML models. In this case, we got images of hotdogs and some other things that aren’t hotdogs.&lt;/li&gt;
&lt;li&gt;We did some data preprocessing and removed broken/useless images and resized all images to the same size.&lt;/li&gt;
&lt;li&gt;We created a simple ConV model and used real-time data augmentation. The results we got didn’t impress us much. So, in the pursuit of happyness and better accuracy, we decided to go a step further.&lt;/li&gt;
&lt;li&gt;We used transfer learning to get better results. From the roster of pre-trained networks, we chose the InceptionV3 network and got the bottleneck features.&lt;/li&gt;
&lt;li&gt;We trained a network similar to the first network on these bottleneck features and got a much better accuracy.&lt;/li&gt;
&lt;li&gt;We plotted some metrics like loss, accuracy and confusion matrix because graphs are cool and useful.&lt;/li&gt;
&lt;li&gt;We sit back and smile at our recent accomplishment and start thinking about what to build next.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So that’s it. I created a pretty good classifier to tell me if something is a hotdog or not. And as Jian-Yang believed, this is the next billion-dollar idea!&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;“It’s intoxicating. Don’t act like it’s not magical. It is!” — Jared&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Yashvardhan Jain</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://j-yash.github.io/neuron/images/post3_main.jpeg" /><media:content medium="image" url="https://j-yash.github.io/neuron/images/post3_main.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Activation Functions Demystified</title><link href="https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html" rel="alternate" type="text/html" title="Activation Functions Demystified" /><published>2018-06-26T00:00:00-05:00</published><updated>2018-06-26T00:00:00-05:00</updated><id>https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified</id><content type="html" xml:base="https://j-yash.github.io/neuron/deep%20learning/2018/06/26/activation-functions-demystified.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-06-26-activation-functions-demystified.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;blockquote&gt;&lt;p&gt;With great deep learning resources, comes great deep learning jargon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you’ve been studying deep learning for a while, you must have come across a lot of jargon associated with the field. And for beginners, it can feel pretty overwhelming. So, if you’ve come across the term “&lt;strong&gt;activation functions&lt;/strong&gt;” and find yourself confused, don’t worry. I’ve got you covered.&lt;/p&gt;
&lt;p&gt;Before we start talking about activation functions, let us first understand what exactly an activation is.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So what is an activation?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An activation is just a number. That’s all an activation is. As we know, the core of deep learning is the following equation:&amp;gt; y = W * x + b
Once we calculate this equation, the value that &lt;code&gt;y&lt;/code&gt; holds is called an activation. And so, every node in the neural network that calculates this equation, holds its own value of &lt;code&gt;y&lt;/code&gt; and all those values are called activations. That is all there is to an activation. No magic. No complicated explanation. No hidden concepts. &lt;strong&gt;It’s simply a number&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alright, guy on the Internet. I get it. But what is an activation function then?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An activation function is simply a mathematical function that is applied to an activation in order to introduce some &lt;strong&gt;non-linearity&lt;/strong&gt; in our network. Confused? Read on.&lt;/p&gt;
&lt;p&gt;A neural network is basically a combination of multiple linear and non-linear functions. The &lt;code&gt;y = m * x + b&lt;/code&gt; is a linear function. But, if we only had linear functions in our network in all layers, then the network would simply act as a single layer network. Such a network would not be able to learn much.&lt;/p&gt;
&lt;p&gt;That is why we use activation functions for producing the output of every node in every layer. This introduces non-linearity in our network which enables the network to learn.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Such a combination of linear and non-linear functions makes a neural network capable of approximating anything.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Are there different activation functions that I can use?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are many different types of activation functions that have been used through the ages. But instead of going through all of them in this article, I will explain the 4 most popular and useful activation functions that are currently used in research as well as in the industry.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sigmoid():&lt;/strong&gt; A sigmoid activation function turns an activation into a value between 0 and 1. It is useful for &lt;strong&gt;binary classification problems&lt;/strong&gt; and is mostly used in the final output layer of such problems. Also, sigmoid activation leads to slow gradient descent because the slope is small for high and low values. A sigmoid activation is represented mathematically as the following equation:&amp;gt; Sigmoid(z) = 1 / (1 + exp(-z))&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/neuron/images/copied_from_nb/../images/post2_sigmoid.png&quot; alt=&quot;&quot; title=&quot;Sigmoid activation&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tanh():&lt;/strong&gt; A Tanh activation function turns an activation into a value between -1 and +1. This activation function is better than the sigmoid activation function in most cases because the outputs are normalized. It is a really popular activation function that is used in the hidden layers. Mathematically, it is represented as:&amp;gt; tanh(z) = [exp(z) - exp(-z) ]/[exp(z) + exp(-z)]&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/neuron/images/copied_from_nb/../images/post2_tanh.png&quot; alt=&quot;&quot; title=&quot;Tanh activation&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ReLU():&lt;/strong&gt; A ReLU (Rectified Linear Unit) is a really complex and fancy sounding activation function, but all it does is simply turn negative values to zero. That’s all it does. It is &lt;strong&gt;the most&lt;/strong&gt; popular and effective activation function and is used in the hidden layers. It is the most used activation function. And is the default choice for most neural network layers. Mathematically, it is represented as:&amp;gt; ReLU(z) = max(0,z)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/neuron/images/copied_from_nb/../images/post2_relu.png&quot; alt=&quot;&quot; title=&quot;ReLU activation&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leaky ReLU:&lt;/strong&gt; One problem with ReLU may be that the derivative(slope) for negative values is zero. In some cases, we may not want that. In order to combat that problem, we can use a leaky ReLU. A leaky ReLU makes sure that the slope for negative values is not zero. But mostly, ReLU function will just do fine. Mathematically, it is represented as:&amp;gt; Leaky_ReLU(z) = max(0.01*z, z)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/neuron/images/copied_from_nb/../images/post2_lrelu.png&quot; alt=&quot;&quot; title=&quot;Leaky ReLU activation&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;advantage&lt;/strong&gt; of ReLU and Leaky ReLU is that the derivative (slope) is much greater than zero (for z &amp;gt; 0), and hence, the algorithm will learn much faster as compared to sigmoid or tanh.&lt;/p&gt;
&lt;p&gt;So, these are the activation functions that are used everywhere from research to industry.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But hey, how do I know which one to use?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So, the rules of thumb for choosing activation functions are pretty simple:1. If you need to choose a value between 0 and 1, like in binary classification, then use a Sigmoid activation function. Otherwise, for all other cases, don’t use this.2. ReLU is the default choice for all the other cases. But in some cases, you can use tanh as well. Try playing with both, if you are not sure which one to use.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;That’s it about activation functions. They are used to make the network non-linear and you’ve got a few choices as to which activation function you can use. I hope I have successfully demystified activation functions for you and you understand them a bit better now.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Yashvardhan Jain</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://j-yash.github.io/neuron/images/post2_main.jpg" /><media:content medium="image" url="https://j-yash.github.io/neuron/images/post2_main.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Hitchhiker’s Guide to Neural Networks - An Introduction</title><link href="https://j-yash.github.io/neuron/deep%20learning/2018/02/20/the-hitchhikers-guide-to-neural-networks.html" rel="alternate" type="text/html" title="The Hitchhiker's Guide to Neural Networks - An Introduction" /><published>2018-02-20T00:00:00-06:00</published><updated>2018-02-20T00:00:00-06:00</updated><id>https://j-yash.github.io/neuron/deep%20learning/2018/02/20/the-hitchhikers-guide-to-neural-networks</id><content type="html" xml:base="https://j-yash.github.io/neuron/deep%20learning/2018/02/20/the-hitchhikers-guide-to-neural-networks.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-02-20-the-hitchhikers-guide-to-neural-networks.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Don't Panic!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If you’re one of those people who has been bitten by the bug of “Deep Learning” and wants to learn how to build the neural networks that power deep learning, you have come to the right place(probably?). In this article, I will try to teach you how to build a neural network and also, answer questions as to why we do what we do and why it all works. This will be a long in-depth article, so grab your popcorn, turn up your music and let’s get started.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick Tip:&lt;/strong&gt; It would be more beneficial if you crank up your IDE and code along.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-is-a-neural-network?&quot;&gt;What is a neural network?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-a-neural-network?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;“…a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.” — Dr. Robert Hecht-Nielsen&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In simpler terms, neural networks (or Artificial Neural Networks) are computing systems that are loosely modeled after the biological neurons in our brains.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;blockquote&gt;&lt;p&gt;A neural network can be represented as a graph of multiple interconnected nodes where each connection can be fine-tuned to control how much impact a certain input has on the overall output.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/neuron/images/copied_from_nb/../images/post1_neuralnet.png&quot; alt=&quot;&quot; title=&quot;A simple neural network&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Few points about a neural network:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A single node in this network along with its input and output, may be referred to as a ‘Perceptron’.&lt;/li&gt;
&lt;li&gt;The network shown above is a 2-layer network because we do not count the input layer, by convention.&lt;/li&gt;
&lt;li&gt;The number of nodes in any layer can vary.&lt;/li&gt;
&lt;li&gt;The number of hidden layers can vary too. We can even create a network that has no hidden layers. Although one must keep in mind that just because we can, doesn’t mean we should.&lt;/li&gt;
&lt;li&gt;It’s vital to set up your neural network model perfectly before running it, otherwise you may end up imploding the entire Multiverse! Not really.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If it is still a bit unclear as to what a neural network is, it would only get clearer when we actually build one and see how it works. So, let me stop with the boring theory, and let’s dive into some code.&lt;/p&gt;
&lt;hr /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We will be using Python 3 and NumPy to build our neural network. We use Python 3 because it’s cool (read:popular and has a lot of superpowers in the form of packages and frameworks for machine learning). And NumPy is a scientific computing package for Python that makes it easier to implement a lot of Math (read:makes our lives a bit easier and simpler).&lt;/p&gt;
&lt;p&gt;We will not be using the overly used and super boring “Housing Price Prediction” dataset for our tutorial. Instead, we will be using “Kaggle’s Titanic: Machine Learning from Disaster” dataset to predict who on the sinking Titanic would have survived.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick Tip:&lt;/strong&gt; &lt;em&gt;If you don’t know this already, &lt;a href=&quot;https://www.kaggle.com&quot;&gt;Kaggle&lt;/a&gt; is a paradise for machine learners. It hosts machine learning competitions, tutorials and has thousands of funky datasets that you can use to do cool machine learning stuff. Do check it out!&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-will-we-be-building?&quot;&gt;What will we be building?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-will-we-be-building?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We will be building a 2-Layer neural network with the following specs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Number of input nodes = Number of input features = 6&lt;/li&gt;
&lt;li&gt;Number of nodes in the hidden layer = 4&lt;/li&gt;
&lt;li&gt;Number of Output nodes = 1&lt;/li&gt;
&lt;li&gt;Type of output = Binary (0 — Dead, 1 — Survived)&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-is-our-dataset?&quot;&gt;What is our dataset?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-our-dataset?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The dataset consists of certain details about the passengers aboard the RMS Titanic as well as whether or not they survived. Before going further, I would recommend you clone the github repository (or simply download the files)from &lt;a href=&quot;https://github.com/J-Yash/The-hitchhiker-s-guide-to-neural-networks-an-introduction.git&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can either type the code as we go. Or simply open the &lt;em&gt;“first neural network.py”&lt;/em&gt; file that you just downloaded along with the dataset. I would recommend you type.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;train.csv&lt;/strong&gt; file contains the following details( along with what they mean):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PassengerId : A unique ID to identify each Passenger in our dataset&lt;/li&gt;
&lt;li&gt;Embarked : Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)&lt;/li&gt;
&lt;li&gt;Pclass : Ticket Class&lt;/li&gt;
&lt;li&gt;Name : Name of the passenger&lt;/li&gt;
&lt;li&gt;Sex : Sex/Gender of the passenger&lt;/li&gt;
&lt;li&gt;Age : Age of the passenger&lt;/li&gt;
&lt;li&gt;SibSp : Number of siblings/spouses aboard the Titanic&lt;/li&gt;
&lt;li&gt;Parch : Number of parents/children aboard the Titanic&lt;/li&gt;
&lt;li&gt;Ticket : Ticket Number&lt;/li&gt;
&lt;li&gt;Fare : Passenger Fare&lt;/li&gt;
&lt;li&gt;Cabin : Cabin Number&lt;/li&gt;
&lt;li&gt;Survived : Whether or not they survived (0 = No; 1 = Yes)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The details numbered &lt;strong&gt;1&lt;/strong&gt; through &lt;strong&gt;11&lt;/strong&gt; are called the &lt;em&gt;attributes&lt;/em&gt; or &lt;em&gt;features&lt;/em&gt; of our dataset thus, forming the input of our model. The detail number &lt;strong&gt;12&lt;/strong&gt; is called the &lt;em&gt;output&lt;/em&gt; of our dataset. By convention, &lt;em&gt;features&lt;/em&gt; are represented by &lt;em&gt;X&lt;/em&gt; whereas the &lt;em&gt;output&lt;/em&gt; is represented by &lt;em&gt;Y&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Finally, take out your wand so we could conjure up our neural network. And if you’re a mere muggle like me, then your laptop will just do fine.&lt;/p&gt;
&lt;p&gt;And now, let the coding begin.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The above code imports all the packages and dependencies that we will need. &lt;strong&gt;Pandas&lt;/strong&gt; is a data analysis package for python and will help us in reading our data from the file. &lt;strong&gt;Matplotlib&lt;/strong&gt; is a package for data visualization and will let us plot graphs.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Reading data from the CSV file&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                            &lt;span class=&quot;vm&quot;&gt;__file__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delimiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;quoting&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;#Converting data from Pandas DataFrame to Numpy Arrays&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data_np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Converting string values into numeric values&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;male&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Q&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Creating training and test sets&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;624&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#Normalizing Age&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;624&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;624&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;891&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Normalizing Age&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;624&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;891&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Phew! That’s a lot of code and we haven’t even started building our neural net. A lot of tutorials choose to skip this step and give you the data that can be easily used in our neural network model. But I feel that this is an extremely important step. And hence, I chose to include this &lt;strong&gt;preprocessing&lt;/strong&gt; step into this tutorial. So what exactly is happening? Here’s what.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;def preprocess()&lt;/code&gt; reads the data from the ‘train.csv’ file that we downloaded and creates a Pandas DataFrame, which is a data structure Pandas provides. (No need to worry too much about Pandas and DataFrames right now)&lt;/p&gt;
&lt;p&gt;We take our data from the Pandas DataFrame and create a NumPy array. &lt;code&gt;X&lt;/code&gt; is a NumPy array that contains our attributes and &lt;code&gt;Y&lt;/code&gt; is a NumPy array that contains our outputs. Also, instead of using all the data from the file, we only use 6 attributes, namely(along with their changed representations):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pclass&lt;/li&gt;
&lt;li&gt;Sex : 1 for male; 2 for female&lt;/li&gt;
&lt;li&gt;Age&lt;/li&gt;
&lt;li&gt;Sibsp&lt;/li&gt;
&lt;li&gt;Parch&lt;/li&gt;
&lt;li&gt;Embarked : 1 for C; 2 for Q; 3 for S&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We convert all the data of type string into float values. We represent string values with some numeric value. &lt;strong&gt;WHY?&lt;/strong&gt; &lt;em&gt;This is done because neural networks take only numeric values as inputs. We can’t use strings as inputs. That is the reason why all the data is converted into integer/float values.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, we divide our data into training and test sets. The dataset contains a total of 891 examples. So we divide our data as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Training Set&lt;/strong&gt; ~ 70% of data (624 examples)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test Set&lt;/strong&gt; ~ 30% of data (267 examples)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;X_train&lt;/code&gt; and &lt;code&gt;Y_train&lt;/code&gt; contain the training input and training output respectively.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;X_test&lt;/code&gt; and &lt;code&gt;Y_test&lt;/code&gt; contain the test input and test output respectively.&lt;/p&gt;
&lt;p&gt;The Test Sets contain whether or not the passenger survived.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Now, the question that you might be asking is, “What just happened? Why did we just divide our data into training and test sets? What is the need for this? What exactly are training and test sets? Why isn’t Pizza good for our health???”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To be completely honest, I really don’t know the answer to that last question. I’m sorry. But I can help you with the other questions.&lt;/p&gt;
&lt;p&gt;Before we try to understand the need for different training and test sets, we need to understand what exactly does a neural network does?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A Neural Network finds (or tries to) the correlation between the inputs and the output.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So we train the network on a bunch of data, and the network finds the correlation between the input(attributes) and the output. But how do we know that the network will &lt;strong&gt;generalize&lt;/strong&gt; well? How can we make sure that the network will be able to make predictions (correlate) new data that it has never seen before?&lt;/p&gt;
&lt;p&gt;When you study for your Defence Against the Dark Arts exam at Hogwarts, how do they know that you actually understand the subject matter? Sure, you can answer the ten questions at the back of the chapter because you have studied those questions. But, can you answer new questions based on the same chapter? Can you answer the questions that you have never seen before, but are based on the same subject matter? How do they find that out? What do they do? They &lt;strong&gt;Test&lt;/strong&gt; you. In your final exam.&lt;/p&gt;
&lt;p&gt;In the same way, we use test set to check whether or not our network generalizes well. We train our network on the training set. And then, we test our network on the test data which is completely new to our network. This test data tells us how well our model generalizes.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The goal of the test set is to give you an unbiased estimate of the performance of your final network.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Generally, we use 70% of the total available data as the training data, and the rest 30% as the test data. This, of course, is not a strict rule and is changed according to the circumstances.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Additionally, there is a “Hold Out Cross Validation Set” or “Development Set”. This set is used to check which model works best. Validation set (also called dev set) is an extremely important aspect and it’s creation/use is considered an important practice in Machine Learning. I omitted this from this tutorial for reasons only Lord Voldemort knows (Ok! I got a bit lazy).
Now let’s move forward.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;weight_initialization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_of_hidden_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# For the hidden layer&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_of_hidden_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                                          &lt;span class=&quot;c1&quot;&gt;# A [4x6] weight  matrix&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number_of_hidden_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# A [4x1] bias matrix&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# For the output layer&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;number_of_hidden_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                                           &lt;span class=&quot;c1&quot;&gt;# A [1x4] weight matrix&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# A [1x1] bias matrix&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The function &lt;code&gt;def weight_initialization()&lt;/code&gt; initializes our weights and biases for the hidden layer and the output layer. &lt;code&gt;W1&lt;/code&gt; and &lt;code&gt;W2&lt;/code&gt; are the weights for the hidden layer and the output layer respectively. &lt;code&gt;b1&lt;/code&gt; and &lt;code&gt;b2&lt;/code&gt; are the biases for the hidden layer and the output layer respectively. &lt;code&gt;W1&lt;/code&gt;, &lt;code&gt;W2&lt;/code&gt;, &lt;code&gt;b1&lt;/code&gt;, &lt;code&gt;b2&lt;/code&gt; are NumPy matrices. But, how do we know what the dimensions of these matrices will be?&lt;/p&gt;
&lt;p&gt;Dimensions are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;W⁰ = (n⁰, n¹)&lt;/li&gt;
&lt;li&gt;b⁰ = (n⁰, 1)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;where; 0 = Current Layer, 1 = Previous Layer, n = Number of nodes&lt;/p&gt;
&lt;p&gt;Since, we are creating a 2-Layer neural network with 6 input nodes, 4 hidden nodes and 1 output node, we use the dimensions as shown in the code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why initialize the weights with random values, instead of initializing with zeros (like bias)?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We initialize the weights with random values in order to &lt;strong&gt;break symmetry&lt;/strong&gt;. If we initialize all the weights with the same value (0 or 1), then the signal that will go into each node in the hidden layers will be similar. For example, if we initialize all our weights as 1, then each node gets a signal that is equal to the sum of all the inputs. If we initialize all the weights as 0, then all the nodes will get a signal of 0. Why does this happen? Because the neural network works on the following equation:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Y = W*X + b&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;But-wait,-what-exactly-is-weight-and-bias?&quot;&gt;But wait, what exactly is weight and bias?&lt;a class=&quot;anchor-link&quot; href=&quot;#But-wait,-what-exactly-is-weight-and-bias?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Weight and bias can be looked upon as two knobs that we have access to. Quite like the volume button in our TV, weight and bias can be turned up or turned down. They can be used to control how much effect a certain input will have on the final output. There is an exact relationship between the weight and the error in our prediction, which can be derived mathematically. We won’t go into the math of it, but simply put, weight and bias let us control the impact of each input on the output and hence, let us reduce the total error in our network, thereby increasing the accuracy of the network.&lt;/p&gt;
&lt;p&gt;Moving on.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Z1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Analogous to Y = W*X + b&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Z2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The function &lt;code&gt;def forward_propagation()&lt;/code&gt; defines the forward propagation step of our learning. So what’s happening here?&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Z1&lt;/code&gt; gives us the output of the dot product of weights with the inputs (and added bias). The &lt;code&gt;sigmoid()&lt;/code&gt; &lt;strong&gt;activation function&lt;/strong&gt; is then applied to these outputs, which gives us &lt;code&gt;A1&lt;/code&gt;. This &lt;code&gt;A1&lt;/code&gt; acts as the input to the second layer(output layer) and same steps are repeated. The final output is stored in &lt;code&gt;A2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is what forward propagation is. We propagate through the network and apply our mathematical magic to the inputs. We use our weights and biases to control the impact of each input on the final output.&lt;/p&gt;
&lt;h2 id=&quot;But-what-is-an-activation-function?&quot;&gt;But what is an activation function?&lt;a class=&quot;anchor-link&quot; href=&quot;#But-what-is-an-activation-function?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;An activation function is used on the output of each node in order to introduce some non-linearity in our outputs. It is important to introduce non-linearity because without it, a neural network would just act as a single layer perceptron, no matter how many layers it has. There are several different types of activation functions like sigmoid, ReLu, tanh etc. Sigmoid function gives us the output between 0 and 1. This is useful for binary classification since we can get the probability of each output.&lt;/p&gt;
&lt;p&gt;Once we have done the forward propagation step, it’s time to calculate by how much we missed the actual outputs i.e. the error/loss. Therefore, we compute the cost of the network, which is basically a mean of the loss/error for each training example.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compute_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                    &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;code&gt;def compute_cost()&lt;/code&gt; function calculates the cost of our network. Now remember, do not panic! That one line is loaded so take your time understanding what it is really doing. The formula for computing cost is:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;cost = -1/m[summation(y&lt;em&gt;log(y’) + (1-y)&lt;/em&gt;log(1-y’))]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;where; m=total number of output examples, y=actual output value, y’=predicted output value&lt;/p&gt;
&lt;p&gt;Once we have computed the cost, we start with our dark magic of backpropagation and gradient descent.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;back_propagation_and_weight_updation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dZ2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dW2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dZ2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;db2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dZ2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dZ1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dZ2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dW1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dZ1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;db1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dZ1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db2&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;code&gt;def back_propagation_and_weight_updation()&lt;/code&gt; function is our backpropagation and weight updation step. Well, duh.&lt;/p&gt;
&lt;h2 id=&quot;So-what-is-backpropagation?&quot;&gt;So what is backpropagation?&lt;a class=&quot;anchor-link&quot; href=&quot;#So-what-is-backpropagation?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Backpropagation step is used to find out how much each parameter affected the error. What was the contribution of each parameter in the total cost of our network. This step is the most Calculus-heavy step because we need to calculate the gradients of the final cost function with respect to the inner parameters(Weight and bias). For example, &lt;code&gt;dW2&lt;/code&gt; means the partial derivative(gradient) of the final cost function with respect to &lt;code&gt;W2&lt;/code&gt;(which is the weights of output layer).&lt;/p&gt;
&lt;p&gt;If this part seems scary, and you are as terrified of calculus as most people, don’t worry. Modern frameworks like Tensorflow, Pytorch etc. calculate these derivatives on their own.&lt;/p&gt;
&lt;p&gt;So by backpropagation, we find out &lt;code&gt;dW1&lt;/code&gt;, &lt;code&gt;dW2&lt;/code&gt;, &lt;code&gt;db1&lt;/code&gt;, &lt;code&gt;db2&lt;/code&gt; which tell us by how much we should change our weights and biases in order to reduce the error, thereby reducing the cost of our network. And so we update our weights and biases.&lt;/p&gt;
&lt;h2 id=&quot;So-what-is-Gradient-Descent?&quot;&gt;So what is Gradient Descent?&lt;a class=&quot;anchor-link&quot; href=&quot;#So-what-is-Gradient-Descent?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Iterating over the network again and again, we keep on reducing our cost of the network by going over the forward propagation and backpropagation step again and again. This iterative reduction of cost in order to find the minimum value for the cost function, is called Gradient Descent.&lt;/p&gt;
&lt;h2 id=&quot;And-what-is-learning_rate?&quot;&gt;And what is &lt;code&gt;learning_rate&lt;/code&gt;?&lt;a class=&quot;anchor-link&quot; href=&quot;#And-what-is-learning_rate?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;code&gt;learning_rate&lt;/code&gt; is a &lt;strong&gt;hyperparameter&lt;/strong&gt; that is used to control the speed and steps of gradient descent. We can tune this hyperparameter to make our gradient descent faster or slower in order to avoid mainly two problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Overshooting : This means, we have jumped over the minima and are now stuck in a limbo. The value of cost function will go haywire and increase and decrease and do all sorts of things but won’t reach the global minima.&lt;/li&gt;
&lt;li&gt;Super long training time : The network takes so long to train that your body turns into a skeleton.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To prevent these two conditions from happening, we use the learning rate which is one of the many hyperparameters.&lt;/p&gt;
&lt;p&gt;So that’s it, that’s all we have to do to train a network. After this, we use different evaluation metrics to test how good our network performs. And then, if we’re feeling fancy, we can use graphs to visualize what we have done.&lt;/p&gt;
&lt;p&gt;So, tying all these function together, we write the final function.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;vm&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# STEP 1: LOADING AND PREPROCESSING DATA&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# STEP 2: INITIALIZING WEIGHTS AND BIASES&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;number_of_hidden_nodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_initialization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;number_of_hidden_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Setting the number of iterations for gradient descent&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_of_iterations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50000&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;all_costs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_of_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# STEP 3: FORWARD PROPAGATION&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# STEP 4: COMPUTING COST&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;all_costs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# STEP 5: BACKPROPAGATION AND PARAMETER UPDATTION&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;back_propagation_and_weight_updation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Cost after iteration &amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;: &amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# STEP 6: EVALUATION METRICS&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# To Show accuracy of our training set&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Accuracy for training set: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
               &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;%&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# To show accuracy of our test set&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Accuracy for test set: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%d&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;%&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# STEP 7: VISUALIZING EVALUATION METRICS&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Plot graph for gradient descent&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_costs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Cost&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of Iterations&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We do 50,000 iterations of our training. &lt;code&gt;num_of_iterations&lt;/code&gt; is also a hyperparameter. Once trained, we calculate the accuracy of our trained network on the training data. This comes out to be 79%.&lt;/p&gt;
&lt;p&gt;Then we check how good our network is at generalizing by using our test data. This accuracy hits 81%.&lt;/p&gt;
&lt;p&gt;The cost function decreases as shown in the following graph. Because graphs are cool!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/neuron/images/copied_from_nb/../images/post1_plot.png&quot; alt=&quot;&quot; title=&quot;Gradient Descent&quot; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the cost of our network reduces drastically at first and then reduces slowly, before being plateaued.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When you run this code on your machine, your graph might be a bit different because weights are initialized randomly. But still, you will get a nearly 80% accuracy on your training and test sets.&lt;/p&gt;
&lt;p&gt;So, we are able to predict who would have survived the sinking of the RMS Titanic with an 81% accuracy. Not bad for a simple 2-layer model. &lt;strong&gt;Congratulations. You have built your very first neural network.&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;You’re a Wizard now!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Modern frameworks like Tensorflow and PyTorch do a lot of things for us and make it easier to build deep learning models. But it is important to understand what exactly is going on behind-the-scenes. I hope you found this article useful. And understand neural networks a bit better than you did before reading this article.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Do-it-Yourself:&lt;/strong&gt; Try changing the hyperparameters(learning rate, number of iterations, and number of hidden nodes) to see how it affects the network. Also, try changing the activation function in &lt;code&gt;A1&lt;/code&gt; from &lt;code&gt;sigmoid&lt;/code&gt; to &lt;code&gt;tanh&lt;/code&gt;.&lt;/em&gt;
&lt;em&gt;Now, how do we improve this network? How do we make this better? There are a lot of tips, tricks and techniques that can be used and you can learn about them at the University of Internet.&lt;/em&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Yashvardhan Jain</name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://j-yash.github.io/neuron/images/post1_main.jpg" /><media:content medium="image" url="https://j-yash.github.io/neuron/images/post1_main.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>